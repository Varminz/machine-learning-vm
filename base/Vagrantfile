# -*- mode: ruby  encoding: iso-8859-1  -*-
# vi: set ft=ruby :
# **************************************************************************
# Create a CentOS 6 virtual machine with all the software needed to
# run Spark + notebooks
# **************************************************************************

vagrant_command = ARGV[0]

# --------------------------------------------------------------------------
# Variables defining the installation of Spark


# The version of Spark we will download & install
spark_version = '2.1.0'
spark_name = 'spark-' + spark_version + '-bin-custom'
#spark_name = 'spark-' + spark_version + '-bin-hadoop2.6'

# The place where Spark will be deployed inside the local machine
# There is usually no need to change this
spark_basedir = '/opt/spark'

# The artifact repository where some custom-built files are kept
repo_base = 'http://artifactory.hi.inet/artifactory/vagrant-machinelearning/buildfiles/'

# --------------------------------------------------------------------------
# Some variables that affect Vagrant execution

# Check the command requested
vagrant_command = ARGV[0]

# Conditionally activate some provision sections
provision_run_tf  = ENV['PROVISION_TF'] == '1' || \
        (vagrant_command == 'provision' && ARGV.include?('60.tf'))
provision_run_dl = (ENV['PROVISION_DL'] == '1') || \
        (vagrant_command == 'provision' && ARGV.include?('61.dl'))
provision_run_clean = (ENV['PROVISION_CLEAN'] == '1') || \
        (vagrant_command == 'provision' && ARGV.include?('70.clean'))


# --------------------------------------------------------------------------
# Vagrant configuration

# The "2" in Vagrant.configure configures the configuration version
Vagrant.configure(2) do |config|

  # This is to help later when packaging: don't change the insecure key
  config.ssh.insert_key = false
  #config.ssh.insert_key = true
  #config.ssh.forward_agent = true
  #config.ssh.username = "ubuntu"
  
  config.vm.define "vgr-spark-base32" do |vgrspark|
     
    # The most common configuration options are documented and commented below.
    # For a complete reference, please see the online documentation at
    # https://docs.vagrantup.com.

    # The base box we will be using. 
    # Available at https://atlas.hashicorp.com/search
    #vgrspark.vm.box = "bento/centos-7.3"
    #vgrspark.vm.box = "ubuntu/xenial32"
    vgrspark.vm.box = "bento/ubuntu-16.04-i386"
    #vgrspark.vm.box_version = "2.2.9"
    # v. 2.3.0 has a regression problem with some VirtualBox versions
    # see https://github.com/chef/bento/issues/688
    # A workaround is included below

    # Disable automatic box update checking. If you disable this, then
    # boxes will only be checked for updates when the user runs
    # `vagrant box outdated`. This is not recommended.
    # vgrspark.vm.box_check_update = false

    # Put into the usual synced folder a subdirectory
    vgrspark.vm.synced_folder ".", "/vagrant", disabled: true
    vgrspark.vm.synced_folder "vmfiles", "/vagrant", 
    disabled: false
    # owner: spark_username
    #auto_mount: false
  
    # Customize the virtual machine: hostname & RAM
    vgrspark.vm.hostname = "vgr-spark-base32"
    vgrspark.vm.provider :virtualbox do |vb|
      # Set the hostname in the provider
      vb.name = vgrspark.vm.hostname.to_s
      # Customize the amount of memory on the VM
      vb.memory = "1024"
      # Display the VirtualBox GUI when booting the machine
      #vb.gui = true
      # A patch for a problem in VirtualBox -- fixed in VB 5.0.28 and 5.1.6
      # see https://github.com/chef/bento/issues/688
      vb.customize ["modifyvm", :id, "--cableconnected1", "on"]
    end

    # Networking
    # Declare a public network
    #vgrspark.vm.network "public_network", type: "dhcp", bridge: 'Realtek PCIe GBE Family Controller', :mac => "08002710A7ED"

    # Create a forwarded port mapping which allows access to a specific port
    # within the machine from a port on the host machine. In the example below,
    # accessing "localhost:8080" will access port 80 on the guest machine.
    #vgrspark.vm.network "forwarded_port", guest: port_ipython, host: port_ipython
    #vgrspark.vm.network :forwarded_port, 
    #host: 4040, 
    #guest: 4040, 
    #auto_correct: true                 # Spark UI (Driver)

    # Share an additional folder to the guest VM. The first argument is
    # the path on the host to the actual folder. The second argument is
    # the path on the guest to mount the folder. And the optional third
    # argument is a set of non-required options.
    # vgrspark.vm.synced_folder "../data", "/vagrant_data"

    # Define a Vagrant Push strategy for pushing to Atlas. Other push strategies
    # such as FTP and Heroku are also available. See the documentation at
    # https://docs.vagrantup.com/v2/push/atlas.html for more information.
    # vgrspark.push.define "atlas" do |push|
    #   push.app = "YOUR_ATLAS_USERNAME/YOUR_APPLICATION_NAME"
    # end
    # Push to artifactory
    #curl -i -u<USERNAME>:<API_KEY> -T <PATH_TO_FILE> "http://artifactory.hi.inet:8081/artifactory/vagrant-machinelearning/{vagrantBoxName.box};box_name={name};box_provider={provider};box_version={version}"

    vgrspark.vm.post_up_message = "**** The Vagrant Spark base machine is up"

    # ---------------------------------------------------------------------

    # https://github.com/mitchellh/vagrant/issues/1673
    vgrspark.vm.provision "00.fix-tty",
    type: "shell",
    privileged: true,
    inline: "sed -i '/tty/!s/mesg n/tty -s \\&\\& mesg n/' /root/.profile"
     
    # Install some base CentOS software
    # (inc. development environments we need to install some Python packages)
    vgrspark.vm.provision "01.base",
    type: "shell",
    privileged: true,
    inline: <<-SHELL
     apt-get update -y
     # Make some subdirectories in the vagrant home dir
     sudo -u vagrant -i mkdir bin install tmp
     # Install some basic packages + dev libraries to compile R/Python pkgs
     #XXXapt install gcc gcc-c++ freetype-devel libpng-devel libffi-devel
     #XXXapt install libopenblas-dev
     # General utility programs
     apt-get install -y emacs-nox virtualenv
     # Other
     #XXX(cd /etc/yum.repos.d; wget http://www.graphviz.org/graphviz-rhel.repo)
     apt-get install -y graphviz
     # customize locales
     #locale-gen "es_ES.UTF-8" "es_AR.UTF-8" "es_CL.UTF-8" "pt_BR.UTF-8" "de_DE.UTF-8"
     # dpkg-reconfigure locales
    SHELL

    # .........................................
    vgrspark.vm.provision "02.jdk",
    type: "shell", 
    privileged: true, 
    inline: "apt-get install -y openjdk-8-jdk-headless"
   # http://stackoverflow.com/questions/10268583/downloading-java-jdk-on-linux-via-wget-is-shown-license-page-instead

    # Install R from EPEL
    vgrspark.vm.provision "03.R", 
    type: "shell", 
    privileged: true, 
    inline: <<-SHELL
     apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9
     add-apt-repository -u "deb http://cran.es.r-project.org/bin/linux/ubuntu xenial/"
     apt-get install -y r-recommended
     #(r-base-dev)
    SHELL

    # Install some R packages & the IR kernel for Jupyter
    vgrspark.vm.provision "04.Rpkg", 
    type: "shell", 
    privileged: true, 
    inline: <<-SHELL
     RDIR=$(Rscript -e "for (p in .libPaths()) { if( startsWith(p,'/usr/local')) { cat(p); break; }}")
     mkdir -p $RDIR
     echo "Installing R packages: rmarkdown, magrittr, dplyr, tidyr, data.table, ggplot2"
     apt-get install -y libpq-dev libcurl4-openssl-dev libzmq5 libmariadb-client-lgpl-dev libxml2-dev
     #yum -y install libcurl-devel libxml2-devel czmq-devel mysql-devel postgresql-devel cairo-devel libssh2-devel
     for pkg in "'rmarkdown','magrittr'" \
                "'dplyr','tidyr'" \
                "'data.table','ggplot2'" \
                "'caret'"
     do
         echo -e "\nInstalling R packages: $pkg"
         Rscript -e "install.packages(c($pkg),dependencies=TRUE,repos=c('http://ftp.cixug.es/CRAN/','http://cran.es.r-project.org/'),quiet=FALSE)"
     done

     echo "Installing IRkernel"
     Rscript -e 'install.packages( c("crayon","devtools"),repos=c("http://ftp.cixug.es/CRAN/","http://cran.es.r-project.org/"),quiet=FALSE)'
     Rscript -e 'devtools::install_github( paste0("IRkernel/",c("repr","IRdisplay","IRkernel")) )'

     echo "Installing sparklyr"
     # As of March 2017, we need to install from git to have support for 2.1
     Rscript -e 'devtools::install_github("rstudio/sparklyr")'

     apt-get remove -y libpq-dev libcurl4-openssl-dev libmariadb-client-lgpl-dev libxml2-dev
     #yum erase -y postgresql-devel mysql-devel cairo-devel gl-manpages glib2-devel libssh2-devel
    SHELL

    # .........................................
    # Install Python 2.7 by using the Software Collections (SCL)
#    vgrspark.vm.provision "10.py27.scl", 
#    type: "shell", 
#    inline: <<-SHELL
#     sudo yum -y install centos-release-SCL centos-release-scl
#     sudo yum -y install python27 python27-python-virtualenv
#    SHELL

    # In order to compile code that requires C++11 we need:
    # sudo yum -y install devtoolset-3-gcc-c++ devtoolset-3-gcc-gfortran
    # scl enable devtoolset-3 bash

    # .........................................
    # Create a Python 2.7 virtualenv and install wrappers to point there
    vgrspark.vm.provision "11.py27.venv",
    type: "shell", 
    privileged: false,
    inline: <<-SHELL
     sudo rm -rf /opt/ipnb
     sudo mkdir -m 775 /opt/ipnb
     sudo chown vagrant.vagrant /opt/ipnb

     # Create a virtualenv
     virtualenv /opt/ipnb --no-site-packages

     cd $HOME/bin
     rm -f python python2.7 pip
     ln -s /opt/ipnb/bin/{python,python2.7,pip} .
   SHELL


    # .........................................
    # In the virtualenv, install the Python packages to use in the Notebooks
    # Plus a few ML-related more
    vgrspark.vm.provision "14.py27.pkg",
    type: "shell", 
    keep_color: true,
    privileged: false, 
    inline: <<-SHELL
      sudo apt-get install -y libpython2.7-dev libopenblas-dev libhdf5-dev libfreetype6-dev
      #yum -y install openblas-devel hdf5-devel

      cd /opt/ipnb/bin
      ./pip install --upgrade pip
      ./pip install --upgrade setuptools
      # we install this is to avoid InsecurePlatformWarning messages
      ./pip install 'requests[security]'

      ./pip install pyparsing
      ./pip install matplotlib
      ./pip install ipython
      ./pip install jupyter     
      ./pip install notebook

      ./pip install h5py
      ./pip install tables

      ./pip install pandas
      ./pip install bottleneck
      ./pip install numexpr
      ./pip install statsmodels
      ./pip install scikit-learn
      ./pip install gensim
      ./pip install networkx
      ./pip install mpld3
      ./pip install seaborn

      ./pip install pydot-ng
      ./pip install graphviz

      ./pip install hdfs
      ./pip install xlrd
      ./pip install openpyxl

      # We don't need this anymore
      sudo apt-get remove -y libopenblas-dev libhdf5-dev libfreetype6-dev
      #sudo yum -y erase hdf5-devel

    SHELL

    # .........................................
    # Install Scala
    vgrspark.vm.provision "20.scala",
    type: "shell", 
    privileged: false,
    keep_color: true,    
    args: [ spark_version, spark_name, spark_basedir ],
    inline: <<-SHELL
      # download & install Scala
      cd install
      VERSION=2.11.8
      echo "Downloading Scala $VERSION"
      wget --no-verbose http://downloads.lightbend.com/scala/$VERSION/scala-$VERSION.deb
      sudo dpkg -i scala-$VERSION.deb
    SHELL


    # .........................................
    # Install a pre-built Spark
    # We can either install a version we download from a Spark website mirror, or
    # a locally built custom version
    vgrspark.vm.provision "30.spark",
    type: "shell",
    privileged: false,
    keep_color: true,
    args: [ spark_version, spark_name, spark_basedir, repo_base ],
    inline: <<-SHELL

      # download & install Spark
      sudo bash -c "mkdir -m 775 '$3'; chown vagrant.vagrant '$3'; cd $3; rm -f current; ln -s $2/ current"
      case $2 in
        *custom) 
           file="/vagrant/spark/$2.tgz";;
        *) echo "Downloading $2.tgz"
           cd install
           wget --no-verbose "http://apache.rediris.es/spark/spark-$1/$2.tgz"
           file=$2.tgz;;
      esac
      tar zxvf "$file" -C "$3"
      sudo sh -c "echo 'PATH=\\$PATH:$3/current/bin' > /etc/profile.d/spark-path.sh"

      # Create the directory to place Hadoop config
      mkdir -p "$3/current/conf/hadoop"
      sudo rm -f /etc/hadoop
      sudo ln -s "$3/current/conf/hadoop" /etc/hadoop

      # Ensure we will be able to write to the Notebook log & run directories
      for d in /var/log /var/run; do
        test -d $d/ipnb || sudo mkdir -m 1777 $d/ipnb
      done

      # Add to the R library directory a link to the installed SparkR package
      RDIR=$(Rscript -e "for (p in .libPaths()) { if( startsWith(p,'/usr/local')) { cat(p); break; }}")
      if [ -d $RDIR ]; then
         sudo rm -f "$RDIR/SparkR"
         sudo ln -s "$3/$2/R/lib/SparkR/" "$RDIR"
      fi
    SHELL

    # .........................................
    # No Spark add-ons installed
    #  * spark-csv is now part of Spark 2.x
    #  * GraphFrames & Kafka Streaming can be declared as packages in
    #    Spark config and they will be automatically downloaded

    # .........................................
    # Install the Spark configuration files for the different modes
    vgrspark.vm.provision "32.spark.cfg",
    type: "file",
    source: 'buildfiles/conf',
    destination: '/opt/spark/current'

    # .........................................
    # Install the IPython Spark notebook daemon script
    vgrspark.vm.provision "33.notebook.script", 
    type: "file",
    source: "buildfiles/jupyter-notebook-mgr",
    destination: "/opt/ipnb/bin/jupyter-notebook-mgr"

    # .........................................
    # Install the Systemd unit file to manage the notebook server
    vgrspark.vm.provision "34a.notebook.systemd",
    type: "file",
    source: "buildfiles/notebook.service",
    destination: "/tmp/notebook.service"

    vgrspark.vm.provision "34b.notebook.systemd",
    type: "shell",
    privileged: true,
    inline: 'K=notebook.service; D=/etc/systemd/system; mv /tmp/$K $D; chown root.root $D/$K'

    # .........................................
    # Install the Pyspark script wrapper
    vgrspark.vm.provision "35.pyspark.wrapper",
    type: "file",
    source: "buildfiles/pyspark-ipnb",
    destination: "/opt/ipnb/bin/pyspark-ipnb"

    # .........................................
    # Install the Toree Scala/Spark kernel
    # We use a custom-built version compiled from master, to make sure that
    # PR100 (https://github.com/apache/incubator-toree/pull/100), which fixes
    # syntax highlighting, is included
    vgrspark.vm.provision "36.scala.kernel",
    type: "shell",
    privileged: false,
    keep_color: true,
    args: [ spark_version, spark_name, spark_basedir, repo_base ],
    inline: <<-SHELL
      PKG=toree-0.2.0.dev1.tar.gz
      echo "Installing the (Scala) Spark kernel $PKG"
      # -- From PyPi
      #pip install toree
      # -- From the developer snapshot
      #pip install https://dist.apache.org/repos/dist/dev/incubator/toree/0.2.0/snapshots/dev1/toree-pip/$PKG
      # -- From a local repository
      #cd $HOME/install
      #wget --no-verbose "${4}spark-kernel/$1/$PKG"
      #pip install $PKG
      # -- From a locally available file
      pip install /vagrant/toree/$PKG 
    SHELL


    # .........................................
    # Install some files for notebooks

    # a custom css
    vgrspark.vm.provision "40.notebook.css",
    type: "file",
    source: "buildfiles/style/notebook-custom.css",
    destination: "/opt/ipnb/lib/python2.7/site-packages/notebook/static/custom/custom.css"

    # the notebook extensions
    vgrspark.vm.provision "41.notebook.ext",
    type: 'file',
    source: 'buildfiles/nbextensions',
    destination: '/opt/ipnb/share/jupyter'

    # a slightly modified Jupyter icon
    # (so that it's easier to identify browser tabs belonging to the VM)
    vgrspark.vm.provision "42.icon.jupyter",
    type: "file",
    source: 'buildfiles/style/jupyter-favicon-custom.ico',
    destination: "/opt/ipnb/lib/python2.7/site-packages/notebook/static/base/images/favicon-custom.ico"

    # icons for Pyspark & Scala kernels
    vgrspark.vm.provision "43.icon.kernel",
    type: "file", 
    source: 'buildfiles/style/kernel-icons',
    destination: spark_basedir


    # .........................................
    # the theanorc file
    vgrspark.vm.provision "50a.theanorc",
    type: 'file',
    source: 'buildfiles/theanorc',
    destination: '/tmp/theanorc'

    vgrspark.vm.provision "50b.theanorc",
    type: 'shell',
    privileged: true,
    inline: 'cp -p /tmp/theanorc /etc/theanorc'

    
    if (provision_run_tf)
     # http://stackoverflow.com/questions/33634525/tensorflow-on-32-bit-linux
     vgrspark.vm.provision "60.tf",
     type: 'shell',
     privileged: false,
     inline: <<-SHELL
       mkdir bazel
       cd bazel
       wget https://github.com/bazelbuild/bazel/releases/download/0.4.5/bazel-0.4.5-dist.zip
       unzip bazel-0.4.5-dist.zip
       patch < /vagrant/tensorflow/bazel.diff
       ./compile.sh
       ln -s output/bazel $HOME/bin
       wget https://github.com/tensorflow/tensorflow/archive/v1.0.1.tar.gz
       tar zxvf v1.0.1.tar.gz 
       cd tensorflow-1.0.1/
       grep -Rl "lib64"| xargs sed -i 's/lib64/lib/g'
       # --- there are missing options here ----
       PYTHON_BIN_PATH=/opt/ipnb/bin/python CC_OPT_FLAGS='-march=native' ./configure
       bazel build -c opt --jobs 1 --local_resources 1024,1.0,2.0 --verbose_failures //tensorflow/tools/pip_package:build_pip_package
       bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
       cp -p /tmp/tensorflow_pkg/tensorflow-1.0.1-cp27-cp27mu-linux_i686.whl
      SHELL
    end

    if (provision_run_dl)
      vgrspark.vm.provision "61.dl",
      type: "shell",
      privileged: false,
      keep_color: true,
      inline: <<-SHELL
         sudo apt-get install -y libopenblas-dev
         pip install --upgrade /vagrant/tensorflow/tensorflow-1.0.1*.whl
         pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git
         pip install --upgrade keras quiver
       SHELL
    end

    
    # Clean
    if (provision_run_clean)
      vgrspark.vm.provision "70.clean",
      type: "shell",
      privileged: true,
      inline: <<-SHELL
        echo "Cleaning temporal & installation files"
        HH=/home/vagrant
        rm -rf $HH/.cache/pip $HH/.bash_history $HH/install/*
        rm -rf /opt/ipnb/share/jupyter/nbextensions/.git*
        rm -rf /tmp/Rtmp* /var/tmp/yum*
        apt-get autoclean -y
        apt-get clean -y 
        apt-get autoremove -y

        # Remove bash history for root
        unset HISTFILE
        rm -f /root/.bash_history

        # Cleanup log files
        echo "Removing logfiles"
        find /var/log -type f | while read f; do echo -ne '' > $f; done;

        # Remove all temporal files
        rm -rf /tmp/*

        # Zero free space
        echo "Whiteout root & boot partitions"
        for fs in / /boot/
        do
           count=$(df --sync -kP / | tail -n1  | awk -F ' ' '{print $4}') 
           let count--
           dd if=/dev/zero of=${fs}whitespace bs=1024 count=$count
           rm ${fs}whitespace;
        done
        sync

        # Zero the swap space
        swappart=$(cat /proc/swaps | tail -n1 | awk -F ' ' '{print $1}')
        if [ "$swappart" != "" ]; then
          swapoff $swappart;
          dd if=/dev/zero of=$swappart;
          mkswap $swappart;
          swapon $swappart;
        fi

      SHELL
    end



  end

end
