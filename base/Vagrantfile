# -*- mode: ruby -*-
# vi: set ft=ruby :
# **************************************************************************
# Create a CentOS 6 virtual machine with all the software needed to
# run Spark + notebooks
# **************************************************************************

vagrant_command = ARGV[0]

# --------------------------------------------------------------------------
# Variables defining the installation of Spark


# The version of Spark we will download & install
spark_version = '1.6.1'
spark_name = 'spark-' + spark_version + '-bin-hadoop2.6'

# The place where Spark will be deployed inside the local machine
# There is usually no need to change this
spark_basedir = '/opt/spark'

# The artifact repository where some custom-built files are kept
repo_base = 'http://artifactory.hi.inet/artifactory/vagrant-machinelearning/buildfiles/'

# --------------------------------------------------------------------------
# Vagrant configuration

# The "2" in Vagrant.configure configures the configuration version
Vagrant.configure(2) do |config|

  # This is to help later when packaging: don't change the insecure key
  config.ssh.insert_key = false

  config.vm.define "vgr-tid-spark-base64" do |vgrspark|
    
    # The most common configuration options are documented and commented below.
    # For a complete reference, please see the online documentation at
    # https://docs.vagrantup.com.

    # The base box we will be using. 
    # Available at https://atlas.hashicorp.com/search
    vgrspark.vm.box = "bento/centos-6.7"

    # Disable automatic box update checking. If you disable this, then
    # boxes will only be checked for updates when the user runs
    # `vagrant box outdated`. This is not recommended.
    # vgrspark.vm.box_check_update = false

    # Put into the usual synced folder a subdirectory
    vgrspark.vm.synced_folder ".", "/vagrant", disabled: true
    vgrspark.vm.synced_folder "vmfiles", "/vagrant", 
    disabled: false
    # owner: spark_username
    #auto_mount: false
  
    # Customize the virtual machine: hostname & RAM
    vgrspark.vm.hostname = "vgr-tid-spark-base64"
    vgrspark.vm.provider :virtualbox do |vb|
      # Set the hostname in the provider
      vb.name = vgrspark.vm.hostname.to_s
      # Customize the amount of memory on the VM
      vb.memory = "1024"
      # Display the VirtualBox GUI when booting the machine
      #vb.gui = true
    end

    # Networking
    # Declare a public network
    #vgrspark.vm.network "public_network", type: "dhcp", bridge: 'Realtek PCIe GBE Family Controller', :mac => "08002710A7ED"

    # Create a forwarded port mapping which allows access to a specific port
    # within the machine from a port on the host machine. In the example below,
    # accessing "localhost:8080" will access port 80 on the guest machine.
    #vgrspark.vm.network "forwarded_port", guest: port_ipython, host: port_ipython
    #vgrspark.vm.network :forwarded_port, 
    #host: 4040, 
    #guest: 4040, 
    #auto_correct: true                 # Spark UI (Driver)

    # Share an additional folder to the guest VM. The first argument is
    # the path on the host to the actual folder. The second argument is
    # the path on the guest to mount the folder. And the optional third
    # argument is a set of non-required options.
    # vgrspark.vm.synced_folder "../data", "/vagrant_data"

    # Define a Vagrant Push strategy for pushing to Atlas. Other push strategies
    # such as FTP and Heroku are also available. See the documentation at
    # https://docs.vagrantup.com/v2/push/atlas.html for more information.
    # vgrspark.push.define "atlas" do |push|
    #   push.app = "YOUR_ATLAS_USERNAME/YOUR_APPLICATION_NAME"
    # end
    # Push to artifactory
    #curl -i -u<USERNAME>:<API_KEY> -T <PATH_TO_FILE> "http://artifactory.hi.inet:8081/artifactory/vagrant-machinelearning/{vagrantBoxName.box};box_name={name};box_provider={provider};box_version={version}"

    vgrspark.vm.post_up_message = "**** The Vagrant Spark base machine is up"

    # ---------------------------------------------------------------------

    # Install some base CentOS software
    # (inc. development environments we need to install some Python packages)
    vgrspark.vm.provision "01.base",
    type: "shell",
    privileged: false,
    inline: <<-SHELL
     # Just in case there are problems with the EPEL repo SSL certificates
     sudo yum upgrade ca-certificates --disablerepo=epel
     # Install some basic packages + dev libraries to compile R/Python pkgs
     sudo yum -y install redhat-lsb-core
     sudo yum -y install gcc gcc-c++ freetype-devel libpng-devel libffi-devel
     sudo yum -y install openblas-devel
     # General utility programs
     sudo yum -y install man nc emacs-nox nano
     # Other
     sudo yum -y install graphviz
     # Make some subdirectories in the vagrant home dir
     mkdir bin install tmp
    SHELL

    # Install R from EPEL
    vgrspark.vm.provision "02.R", 
    type: "shell", 
    privileged: true, 
    inline: <<-SHELL
     yum -y install epel-release
     yum -y install R
    SHELL

    # Install some R packages & the IR kernel for Jupyter
    vgrspark.vm.provision "03.Rpkg", 
    type: "shell", 
    privileged: true, 
    inline: <<-SHELL
     mkdir -p /usr/local/lib/R/library
     echo "Installing R packages: rmarkdown, magrittr, dplyr, tidyr, data.table, ggplot2"
     yum -y install libcurl-devel libxml2-devel czmq-devel mysql-devel postgresql-devel
     for pkg in '"rmarkdown","magrittr"' \
                '"dplyr","tidyr"' \
                '"data.table","ggplot2"' \
                '"caret"'
     do
         echo -e "\nInstalling R packages: $pkg"
         Rscript -e "install.packages(c($pkg),dependencies=TRUE,repos=c('http://ftp.cixug.es/CRAN/','http://cran.es.r-project.org/'),quiet=FALSE)"
     done
     yum erase -y postgresql-devel mysql-devel
     echo "Installing IRkernel"
     Rscript -e 'install.packages( c("rzmq","repr","IRkernel","IRdisplay"),repos=c("http://irkernel.github.io/","http://ftp.cixug.es/CRAN/","http://cran.es.r-project.org/"),quiet=FALSE)'
    SHELL

    # .........................................
    # Install Oracle JDK
    # We download Oracle 8 from oracle.com, with the dark magic needed to 
    # accept their license before downloading
    vgrspark.vm.provision "04.JDK", 
    type: "shell", 
    inline: <<-SHELL
     cd install
     wget --no-verbose --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" "http://download.oracle.com/otn-pub/java/jdk/8u65-b17/jdk-8u65-linux-x64.rpm"
     sudo yum -y install jdk-8u65-linux-x64.rpm
    SHELL
   # http://stackoverflow.com/questions/10268583/downloading-java-jdk-on-linux-via-wget-is-shown-license-page-instead
   # http://download.oracle.com/otn-pub/java/jdk/7u71-b14/jdk-7u71-linux-x64.tar.gz
   # https://www.digitalocean.com/community/tutorials/how-to-install-java-on-centos-and-fedora

    # .........................................
    # Install Python 2.7 by using the Sofware Collections (SCL)
    vgrspark.vm.provision "05.python27-SCL", 
    type: "shell", 
    inline: <<-SHELL
     sudo yum -y install centos-release-SCL centos-release-scl
     sudo yum -y install python27 python27-scipy python27-python-virtualenv
    SHELL

    # In order to compile code that requires C++11 we need:
    # sudo yum -y install devtoolset-3-gcc-c++ devtoolset-3-gcc-gfortran
    # scl enable devtoolset-3 bash

    # .........................................
    # Create a Python 2.7 virtualenv and install wrappers to point there
    # Since Python 2.7 has been installed with SCL, we need extra magic with 
    # LD_LIBRARY_PATH to make it run
    vgrspark.vm.provision "06.python27.venv",
    type: "shell", 
    privileged: false,
    inline: <<-SHELL
     sudo rm -rf /opt/ipnb
     sudo mkdir -m 775 /opt/ipnb
     sudo chown vagrant.vagrant /opt/ipnb

     # Create a virtualenv
     scl enable python27 'virtualenv /opt/ipnb --no-site-packages'

     # Create a wrapper to execute virtualenv scripts
     mkdir /opt/ipnb/bin/ext
     cd /opt/ipnb/bin/ext
     rm -f ipnb python2.7 pip
     echo 'PATH=/opt/ipnb/bin:$PATH LD_LIBRARY_PATH=/opt/rh/python27/root/usr/lib64$LD_LIBRARY_PATH${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH} exec /opt/ipnb/bin/$(basename $0) "$@"' > wrap
     chmod +x wrap
     # Install some aliases for the wrapper
     for s in python2.7 pip
     do
        ln -s wrap $s 
     done

     # Make some aliases available in the path for the vagrant user
     cd $HOME/bin
     rm python2.7 pip
     ln -s /opt/ipnb/bin/ext/{python2.7,pip} .
   SHELL

    # .........................................
    # In the virtualenv, install the Python libraries to use in the Notebooks
    # Plus a few ML-related more
    vgrspark.vm.provision "07.notebook",
    type: "shell", 
    keep_color: true,
    privileged: false, 
    inline: <<-SHELL

      sudo yum -y install hdf5-devel

      cd /opt/ipnb/bin/ext
      ./pip install --upgrade pip
      ./pip install --upgrade setuptools
      # we install this is to avoid InsecurePlatformWarning messages
      ./pip install 'requests[security]'

      ./pip install pyparsing
      ./pip install matplotlib
      ./pip install ipython
      ./pip install jupyter     

      ./pip install h5py
      ./pip install tables

      ./pip install pandas
      ./pip install statsmodels
      ./pip install mpld3
      ./pip install seaborn
      ./pip install scikit-learn
      ./pip install gensim
      ./pip install networkx

      ./pip install theano
      ./pip install keras
      ./pip install pydot-ng

      ./pip install hdfs
      ./pip install xlrd
      ./pip install openpyxl

      # We don't need this anymore
      sudo yum -y erase hdf5-devel openblas-devel

      for s in ipython jupyter jupyter-notebook hdfscli; do
        rm -f $s; ln -s wrap $s
      done
    SHELL

    # .........................................
    # Install Scala
    vgrspark.vm.provision "08.scala",
    type: "shell", 
    privileged: false,
    keep_color: true,    
    args: [ spark_version, spark_name, spark_basedir ],
    inline: <<-SHELL
      # download & install Scala
      cd install
      wget --no-verbose http://downloads.typesafe.com/scala/2.10.5/scala-2.10.5.rpm
      sudo yum -y install scala-2.10.5.rpm
    SHELL


    # .........................................
    # Install Spark
    vgrspark.vm.provision "10.spark",
    type: "shell",
    privileged: false,
    keep_color: true,
    args: [ spark_version, spark_name, spark_basedir, repo_base ],
    inline: <<-SHELL

      # download & install Spark
      cd install
      echo "Downloading $2.tgz"
      wget --no-verbose "http://apache.rediris.es/spark/spark-$1/$2.tgz"
      sudo bash -c "mkdir -m 775 '$3'; chown vagrant.vagrant '$3'; cd $3; rm -f current; ln -s $2/ current"
      tar zxvf "$2.tgz" -C "$3"
      sudo sh -c "echo 'PATH=\\$PATH:$3/current/bin' > /etc/profile.d/spark-path.sh"

      # Create the directory to place Hadoop config
      mkdir -p "$3/current/conf/hadoop"
      sudo rm -f /etc/hadoop
      sudo ln -s "$3/current/conf/hadoop" /etc/hadoop

      # Ensure we will be able to write to the Notebook log & run directories
      for d in /var/log /var/run; do
        test -d $d/ipnb || sudo mkdir -m 1777 $d/ipnb
      done

      # Add to the R library a link to the installed SparkR package
      DIR=/usr/local/lib/R/library
      if [ -d $DIR ]; then
         sudo rm -f "$DIR/SparkR"
         sudo ln -s "$3/$2/R/lib/SparkR/" "$DIR"
      fi
    SHELL

    # .........................................
    # Install some Spark add-ons
    vgrspark.vm.provision "11.spark-ext",
    type: "shell",
    privileged: false,
    keep_color: true,
    args: [ spark_version, spark_name, spark_basedir, repo_base ],
    inline: <<-SHELL

      EXT_DIR="$3/current/lib/ext"
      mkdir -p $EXT_DIR

      # download & install the Spark Streaming Kafka artifact
      ARTIFACT=spark-streaming-kafka-assembly_2.10
      echo ". Downloading $ARTIFACT"
      wget --no-verbose http://search.maven.org/remotecontent?filepath=org/apache/spark/$ARTIFACT/$1/$ARTIFACT-$1.jar -O $ARTIFACT-$1.jar
      mv $ARTIFACT-$1.jar $EXT_DIR

      # download & install the Spark CSV artifact, plus its dependency
      ARTIFACT_VERSION=1.4.0
      ARTIFACT=spark-csv_2.10-$ARTIFACT_VERSION
      echo ". Downloading $ARTIFACT"
      wget --no-verbose http://central.maven.org/maven2/com/databricks/spark-csv_2.10/$ARTIFACT_VERSION/$ARTIFACT.jar
      mv $ARTIFACT.jar $EXT_DIR
      ARTIFACT_VERSION=1.2
      ARTIFACT=commons-csv-$ARTIFACT_VERSION
      wget --no-verbose http://central.maven.org/maven2/org/apache/commons/commons-csv/$ARTIFACT_VERSION/$ARTIFACT.jar
      mv $ARTIFACT.jar $EXT_DIR

      ARTIFACT_VERSION=0.1.0-spark1.6
      ARTIFACT=graphframes-$ARTIFACT_VERSION
      echo ". Downloading $ARTIFACT"
      wget --no-verbose http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/$ARTIFACT_VERSION/$ARTIFACT.jar
      mv $ARTIFACT.jar $EXT_DIR

    SHELL

    # .........................................
    # Install the Spark configuration files for the different modes
    vgrspark.vm.provision "12.config",
    type: "file",
    source: 'buildfiles/conf',
    destination: '/opt/spark/current'

    # .........................................
    # Install the IPython Spark notebook daemon script
    vgrspark.vm.provision "13.daemon-script", 
    type: "file",
    source: "buildfiles/jupyter-notebook-mgr",
    destination: "/opt/ipnb/bin/ext/jupyter-notebook-mgr"

    # .........................................
    # Install the Pyspark script wrapper
    vgrspark.vm.provision "14.pyspark-wrapper",
    type: "file",
    source: "buildfiles/pyspark-ipnb",
    destination: "/opt/ipnb/bin/ext/pyspark-ipnb"

    # .........................................
    # Install the Toree Scala/Spark kernel
    vgrspark.vm.provision "15.scala-kernel",
    type: "shell",
    privileged: false,
    keep_color: true,
    args: [ spark_version, spark_name, spark_basedir, repo_base ],
    inline: <<-SHELL
      # Install the (Scala) Spark kernel
      pip install toree
      #cd $HOME/install
      #PKG=toree-0.1.0.dev5.tar.gz
      ##wget --no-verbose "${4}spark-kernel/$1/$PKG"
      #pip install /vagrant/$PKG 
    SHELL

    vgrspark.vm.provision "16.kernel-icons",
    type: "file", 
    source: 'buildfiles/style/kernel-icons',
    destination: spark_basedir

    # .........................................
    # Install a custom css for notebooks
    vgrspark.vm.provision "20.notebook-css",
    type: "file",
    source: "buildfiles/style/notebook-custom.css",
    destination: "/opt/ipnb/lib/python2.7/site-packages/notebook/static/custom/custom.css"

    # Install a slightly modified Jupyter icon
    # (so that it's easier to identify browser tabs belonging to the VM)
    vgrspark.vm.provision "21.jupyter-icon",
    type: "file",
    source: 'buildfiles/style/jupyter-favicon-custom.ico',
    destination: "/opt/ipnb/lib/python2.7/site-packages/notebook/static/base/images/favicon-custom.ico"

    # .........................................
    # Install the notebook extensions

    vgrspark.vm.provision "22.notebook.ext",
    type: 'file',
    source: 'buildfiles/nbextensions',
    destination: '/opt/ipnb/share/jupyter'

    vgrspark.vm.provision "23.clean",
    type: "shell",
    inline: "rm -rf /opt/ipnb/share/jupyter/nbextensions/.git*"

    # .........................................
    # Install the additional packages for the AI course
    # Do it only if the environment variable PROVISION_NBCONVERT has a 1 value
    if (ENV['PROVISION_AI'] == '1')
      vgrspark.vm.provision "11.ai",
        type: "shell",
        run: "never",
        keep_color: true,
        inline: <<-SHELL
          echo "Installing AI packages"

          sudo yum -y install python27-tkinter
          cd /opt/ipnb/bin/ext
          ./pip install nltk graphviz
          ./pip install https://github.com/paulovn/sparql-kernel/archive/master.zip
          ./pip install https://github.com/paulovn/aiml-chatbot-kernel/archive/master.zip
        SHELL
    end

  end

end
