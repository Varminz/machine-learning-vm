#!/bin/bash
#
# Starts a Spark notebook
#
# chkconfig: 345 87 13
# description: Spark notebook process
#
### BEGIN INIT INFO
# Provides:          spark-notebook
# Short-Description: Spark IPython notebook
# Default-Start:     3 4 5
# Default-Stop:      0 1 2 6
# Required-Start:    $syslog $remote_fs
# Required-Stop:     $syslog $remote_fs
# Should-Start:
# Should-Stop:
### END INIT INFO

# Source function library.                                     
. /lib/lsb/init-functions

# Read the name of the config file to use, and slurp it
CFGNAME=/etc/sysconfig/spark-notebook-config
SPARK_MODE=$(<$CFGNAME)
if [ -f "${CFGNAME}-${SPARK_MODE}" ] ;then 
    set -a
    . ${CFGNAME}-${SPARK_MODE}
    set +a
fi


DESC="Spark Notebook"
DAEMON="spark-notebook"
VARDIR="/var/run/spark"
PIDFILE="$VARDIR/$DAEMON.pid"
LOCKDIR="/var/lock/subsys"
LOCKFILE="$LOCKDIR/$DAEMON"

NOTEBOOK_USER=${NOTEBOOK_USER:-spark}
NOTEBOOK_SCRIPT=${NOTEBOOK_SCRIPT:-/home/${NOTEBOOK_USER}/bin/pyspark-nb}

RETVAL_SUCCESS=0

STATUS_RUNNING=0
STATUS_DEAD=1
STATUS_DEAD_AND_LOCK=2
STATUS_NOT_RUNNING=3
STATUS_OTHER_ERROR=102

RETVAL=0
SLEEP_TIME=5
PROC_NAME=$(basename $NOTEBOOK_SCRIPT)

EXEC_PATH=$NOTEBOOK_SCRIPT
EXEC_DIR=""
DAEMON_FLAGS=""

[ -d "$VARDIR" ]  || install -d -m 0755 -o $NOTEBOOK_USER -g $NOTEBOOK_USER $VARDIR 1>/dev/null 2>&1 || :
[ -d "$LOCKDIR" ] || install -d -m 0755 $LOCKDIR 1>/dev/null 2>&1 || :


start() {
    [ -x $EXE_FILE ] || exit $ERROR_PROGRAM_NOT_INSTALLED
    log_success_msg "Starting $DESC (${DAEMON}): "

    checkstatusofproc
    status=$?
    if [ "$status" -eq "$STATUS_RUNNING" ]; then
        log_success_msg "${DESC} is running"
        exit 0
    fi

    LOG_FILE=/var/log/spark/${DAEMON}.out
    runuser -s /bin/bash $NOTEBOOK_USER -c "nohup nice -n 0 $NOTEBOOK_SCRIPT >$LOG_FILE 2>&1 &"' echo $!' > $PIDFILE

    sleep 3

    checkstatusofproc
    RETVAL=$?
    [ $RETVAL -eq $STATUS_RUNNING ] && touch $LOCKFILE
    return $RETVAL
}


stop() {
    log_success_msg "Stopping $DESC (${DAEMON}): "
    killproc -p $PIDFILE pyspark-notebook
    RETVAL=$?

    [ $RETVAL -eq $RETVAL_SUCCESS ] && rm -f $LOCKFILE $PIDFILE
    return $RETVAL
}


restart() {
  stop
  start
}


checkstatusofproc(){
  pidofproc -p $PIDFILE $PROC_NAME > /dev/null
}


checkstatus(){
  checkstatusofproc
  status=$?

  case "$status" in
    $STATUS_RUNNING)
      log_success_msg "${DESC} is running"
      ;;
    $STATUS_DEAD)
      log_failure_msg "${DESC} is dead and pid file exists"
      ;;
    $STATUS_DEAD_AND_LOCK)
      log_failure_msg "${DESC} is dead and lock file exists"
      ;;
    $STATUS_NOT_RUNNING)
      log_failure_msg "${DESC} is not running"
      ;;
    *)
      log_failure_msg "${DESC} status is unknown"
      ;;
  esac
  return $status
}


condrestart(){
  [ -e $LOCKFILE ] && restart || :
}


check_for_root() {
  if [ $(id -ur) -ne 0 ]; then
    echo 'Error: root user required'
    echo
    exit 1
  fi
}


usage() {
    echo $"Usage: $0 {start|stop|status|restart|try-restart|condrestart}"
    echo $"       $0 config-mode (local | yarn )"
    echo $"       $0 config-yarn <yarn-resource-manager>"
    exit 1
}


service() {
  case "$1" in
    start)
      check_for_root
      start
      ;;
    stop)
      check_for_root
      stop
      ;;
    status)
      checkstatus
      RETVAL=$?
      ;;
    restart)
      check_for_root
      restart
      ;;
    condrestart|try-restart)
      check_for_root
      condrestart
      ;;
    config-mode)
      test "$2" = "local" -o "$2" = "yarn" || usage
      check_for_root
      test -f "${CFGNAME}-$2" || { echo "can't find Spark config ${CFGNAME}-$2"; exit 1; }
      stop
      echo "$2" > ${CFGNAME}
      log_success_msg "Configuring Spark for mode: $2 "
      start
      ;;
    config-yarn)
      check_for_root
      test -f "${CFGNAME}-yarn" || { echo "can't find Spark config ${CFGNAME}-yarn"; exit 1; }
      test "$2" || usage
      sed "s@{YARN-RM-HOSTNAME}@$2@" /vagrant/files/yarn-site.tpl >/opt/hadoop/etc/yarn-site.xml
      ;;
    *)
      usage
      ;;
  esac
}

service "$@"

exit $RETVAL
