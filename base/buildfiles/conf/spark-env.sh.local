# Environment variables to add to all Spark processes
# ---------------------------------------------------

# In local mode we do not want set Hadoop config, else the default FS 
# would point to HDFS
unset HADOOP_CONF_DIR
#HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/opt/spark/current/conf/hadoop}

# This enables netlib-java to use the optimized BLAS libraries
# For this to work, the Spark distribution needs to contain the native reference
# implementation (which binary pre-built packages do not usually include)
LD_PRELOAD=/usr/lib/libopenblas.so

# Set the Python to use for executors
PYSPARK_PYTHON=/opt/ipnb/bin/python

# Default arguments for job submission
PYSPARK_SUBMIT_ARGS='--master local[*] --driver-memory 1536M --num-executors 2 --executor-cores 2 --executor-memory 1g'
